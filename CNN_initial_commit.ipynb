{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''TO DO:\n",
    "\n",
    "vectorise validation set \n",
    "check gradients function\n",
    "add pooling and backprop for that\n",
    "add bias units backprop\n",
    "add max pooling backprop\n",
    "add multiple filters \n",
    "add multiple outputs \n",
    "add multiple channels \n",
    "add learning curve functionality \n",
    "review pooling function and compare to own pooling method\n",
    "fix variable nomenclature\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# np.random.seed(0)\n",
    "from itertools import repeat\n",
    "import keras\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "cond=np.any([y_train_full==1,y_train_full==3],0)\n",
    "X_train_full=X_train_full[cond]\n",
    "y_train_full=y_train_full[cond]\n",
    "X_test=X_test[np.any([y_test==1,y_test==3],0)]\n",
    "y_test=y_test[np.any([y_test==1,y_test==3],0)]\n",
    "y_train_full=(y_train_full==3).astype(int)\n",
    "y_test=(y_test==3).astype(int)\n",
    "X_train, X_valid = X_train_full[:-1000], X_train_full[-1000:]\n",
    "y_train, y_valid = y_train_full[:-1000], y_train_full[-1000:]\n",
    "#note that the mean and standard deviation are used to standardise the validation and test sets as well\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "# X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Vectorised convolution operation runtime is 0.03seconds on 28 by 28 grey scale image and 3 by 3 kernel\n",
    "# Tested approximately 2 times slower than Scipy.convolve\n",
    "def convolution(image_array,filter):\n",
    "    filter_size=filter.shape[1]\n",
    "    r=image_array.shape[0]\n",
    "    c=image_array.shape[1]\n",
    "    new_image_array=np.uint16(image_array)\n",
    "    reduced=new_image_array[np.uint16(filter_size/2):r-np.uint16(filter_size/2),np.uint16(filter_size/2):c-np.uint16(filter_size/2)]\n",
    "    reduced_indices=np.argwhere(reduced==0|reduced)\n",
    "    reduced_I=reduced_indices[:,0]\n",
    "    reduced_J=reduced_indices[:,1]\n",
    "    combined_reduced_indices=reduced_I*c+reduced_J\n",
    "\n",
    "    original_I=reduced_I+np.uint16(filter_size/2)\n",
    "    original_J=reduced_J+np.uint16(filter_size/2)\n",
    "    combined_original_indices=original_I*c+original_J\n",
    "\n",
    "    adjustor_I=np.array(np.repeat(range(-1*np.uint16(filter_size/2),np.uint16(filter_size/2)+1),filter_size))\n",
    "    adjustor_J=np.tile(np.array(range(-1*np.uint16(filter_size/2),np.uint16(filter_size/2)+1)),filter_size)\n",
    "    combined_adjustor_indices=adjustor_I*c+adjustor_J\n",
    "\n",
    "    '''we could create an array that goes from 0th element up in 3s to the last element and assign it to a vector called row and do the same for a vector called column\n",
    "    we could filter based on an if statement if the filter is of size 3 we append only elements with either row or column index within size 3.\n",
    "    '''\n",
    "    flattened=image_array.flatten()\n",
    "    tiled_indices=np.reshape(np.tile(combined_original_indices,filter_size*filter_size),(filter_size*filter_size,len(combined_original_indices)))\n",
    "\n",
    "    flattened_indices=combined_adjustor_indices.reshape(filter_size*filter_size,1)+tiled_indices\n",
    "    flattened_image=flattened[flattened_indices]\n",
    "\n",
    "    split_array= np.split(flattened_image,filter_size*filter_size)\n",
    "\n",
    "    flattened_image=np.stack(split_array,axis=-1)\n",
    "\n",
    "\n",
    "    image_matrix=flattened_image.reshape(r-np.uint16(filter_size-1),c-np.uint16(filter_size-1),filter_size,filter_size)\n",
    "    final=filter*image_matrix\n",
    "    result=np.sum(np.sum(final,axis=2),axis=2)\n",
    "    #plt.imshow(absolute_final)\n",
    "    return result,image_matrix\n",
    "\n",
    "def relu(feature_map):\n",
    "    x=feature_map.copy()\n",
    "    #Preparing the output of the ReLU activation function.\n",
    "    return np.maximum(x,0,x)\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "\n",
    "''' __________________Discalimer_______________: Credit for pooling function to Stackoverflow user \"Jason\" \n",
    "https://stackoverflow.com/questions/42463172/how-to-perform-max-mean-pooling-on-a-2d-array-using-numpy'''\n",
    "def pooling(mat,ksize,method='max',pad=False):\n",
    "    '''Non-overlapping pooling on 2D or 3D data.\n",
    "\n",
    "    <mat>: ndarray, input array to pool.\n",
    "    <ksize>: tuple of 2, kernel size in (ky, kx).\n",
    "    <method>: str, 'max for max-pooling, \n",
    "                   'mean' for mean-pooling.\n",
    "    <pad>: bool, pad <mat> or not. If no pad, output has size\n",
    "           n//f, n being <mat> size, f being kernel size.\n",
    "           if pad, output has size ceil(n/f).\n",
    "\n",
    "    Return <result>: pooled matrix.\n",
    "    '''\n",
    "\n",
    "    m, n = mat.shape[:2]\n",
    "    ky,kx=ksize\n",
    "\n",
    "    _ceil=lambda x,y: int(np.ceil(x/float(y)))\n",
    "\n",
    "    if pad:\n",
    "        ny=_ceil(m,ky)\n",
    "        nx=_ceil(n,kx)\n",
    "        size=(ny*ky, nx*kx)+mat.shape[2:]\n",
    "        mat_pad=numpy.full(size,np.nan)\n",
    "        mat_pad[:m,:n,...]=mat\n",
    "    else:\n",
    "        ny=m//ky\n",
    "        nx=n//kx\n",
    "        mat_pad=mat[:ny*ky, :nx*kx, ...]\n",
    "\n",
    "    new_shape=(ny,ky,nx,kx)+mat.shape[2:]\n",
    "\n",
    "    if method=='max':\n",
    "        result=np.nanmax(mat_pad.reshape(new_shape),axis=(1,3))\n",
    "    else:\n",
    "        result=np.nanmean(mat_pad.reshape(new_shape),axis=(1,3))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if seed not set so W1 and W2 change every time cell is run\n",
    "#_____________________________________________________Initialise weight matrices_________________________________________________________\n",
    "# W1 is convolutional layer (currently 1 filter), W2 is densely connected layer.\n",
    "np.random.seed(0)\n",
    "scaling_parameter=1\n",
    "W1=np.random.rand(3,3)/scaling_parameter  #need to scale weights to avoid vanishing gradients\n",
    "filter_size=3\n",
    "dense_layer_weight_size=X_train.shape[1]-int((filter_size+1)/2)\n",
    "W2=np.random.rand(dense_layer_weight_size,dense_layer_weight_size)\n",
    "# It seems that scaling down convolutional weights by 10,000 whilst leaving the densely connected weight to be small numbers between 0 and 1\n",
    "# leads to good results.\n",
    "W2_flattened=W2.flatten()\n",
    "\n",
    "\n",
    "train_array_loss=[]\n",
    "train_array_accuracy=[]\n",
    "val_loss_array=[]\n",
    "X_train_shuffled,y_train_shuffled=shuffle_in_unison(X_train,y_train)\n",
    "X_valid_shuffled,y_valid_shuffled=shuffle_in_unison(X_valid,y_valid)\n",
    "accuracy=[]\n",
    "# ________________________________________________________________TRAINING THE NETWORK - SGD_________________________________________________\n",
    "n=100#len(y_train) number of iterations to train\n",
    "for i in range(n):\n",
    "\n",
    "  # print(\"iteration :\",str(i+1)+\"/\"+str(n))\n",
    "  # selects different image every time\n",
    "    X=X_train_shuffled[i]\n",
    "    Y=y_train_shuffled[i]\n",
    "\n",
    "\n",
    "    convoluted_1,image_matrix=convolution(X,W1)\n",
    "\n",
    "    relu_1_flattened=relu(convoluted_1.reshape(26*26))\n",
    "\n",
    "\n",
    "  #W2 flattened is  updating\n",
    "\n",
    "    activated=sigmoid(np.dot(relu_1_flattened,W2_flattened))\n",
    "    activated=activated.clip(10**-16,1-10**-16)\n",
    "\n",
    "    loss_function= -Y*np.log(activated)-(1-Y)*np.log(1-activated)\n",
    "\n",
    "\n",
    "\n",
    "    dL_dz=(activated*(1-Y)-(1-activated)*Y)/(activated*(1-activated))\n",
    "\n",
    "    '''gradient checking'''\n",
    "    # dL/dz correct\n",
    "    # dz_dn2 correct\n",
    "    # dz_dW2 correct\n",
    "    # epsilon=10e-10 \n",
    "    # loss_function_plus=-Y*np.log(activated+epsilon)-(1-Y)*np.log(1-activated-epsilon)\n",
    "    # loss_function_minus=-Y*np.log(activated-epsilon)-(1-Y)*np.log(1-activated+epsilon)\n",
    "    # gradient_check=(loss_function_plus-loss_function_minus)/(2*epsilon)\n",
    "    # print(dL_dz,gradient_check,\"difference:\",gradient_check-dL_dz)\n",
    "    dz_dW2=((1-Y)*activated-Y*(1-activated))*relu_1_flattened\n",
    "    dz_dn2=activated*(1-activated)*W2_flattened.reshape(26,26)\n",
    "\n",
    "\n",
    "    image_matrix_reshaped=image_matrix.reshape(3,3,26,26)\n",
    "    dn2_dw1=image_matrix_reshaped*(convoluted_1>0) #to be checked <<<--------------think its correct\n",
    "    \n",
    "    dz_dW1=dn2_dw1*dz_dn2\n",
    "\n",
    "    dL_dW1=dL_dz*(np.sum(np.sum(dz_dW1,axis=2),axis=2))\n",
    "\n",
    "    #layer 2 gradients\n",
    "    dL_dW2=dL_dz*(activated*(1-activated))*relu_1_flattened\n",
    "    #simultaneous weight updates\n",
    "\n",
    "    learning_rate=0.001\n",
    "    W1=W1-learning_rate*dL_dW1\n",
    "\n",
    "    W2_flattened=W2_flattened-learning_rate*dL_dW2\n",
    "\n",
    "\n",
    "    train_array_accuracy.append(int(activated>0.5)==Y)\n",
    "    train_array_loss.append(loss_function)\n",
    "\n",
    "\n",
    "# print(sum(train_array_accuracy)/len(train_array_accuracy))\n",
    "# print(train_array_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#important in plotting curve that loss function is calculated before parameter update\n",
    "# _______________________________________________________Evaluate trained network on Validation Set______________________________________________\n",
    "# want to evaluate synchronously with training - could expand dimension in convolution operation\n",
    "predicted_array=[]\n",
    "\n",
    "val_loss_sum=0\n",
    "m=len(X_valid)\n",
    "for j in range(m):\n",
    "    X_val=X_valid_shuffled[j]\n",
    "    Y_val=y_valid_shuffled[j]\n",
    "    convoluted_1_valid,image_matrix_valid=convolution(X_val,W1)\n",
    "    relu_1_flattened_valid=relu(convoluted_1_valid.reshape(26*26)) \n",
    "    val_activated=sigmoid(np.dot(relu_1_flattened_valid,W2_flattened))\n",
    "    val_activated=val_activated.clip(10**-16,1-10**-16)\n",
    "\n",
    "    val_loss_function= -Y_val*np.log(val_activated)-(1-Y_val)*np.log(1-val_activated)\n",
    "\n",
    "\n",
    "\n",
    "    predicted_array.append(int(val_activated>0.5)==Y_val)\n",
    "    val_loss_sum+=val_loss_function\n",
    "    avg_val_loss=val_loss_sum/m\n",
    "    val_loss_array.append(avg_val_loss)\n",
    "\n",
    "val_accuracy=(np.sum(predicted_array)/m)\n",
    "\n",
    "\n",
    "\n",
    "print(val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "def smoothed_convergence_curve(train_loss,valid_loss,sliding_window):\n",
    "  # choose sliding window to smooth noisy loss curve\n",
    "    cumulative_array_train_loss=[]\n",
    "    cumulative_array_valid_loss=[]   \n",
    "    for i in range(int(len(train_loss)/sliding_window)):\n",
    "        cumulative_array_train_loss.append(sum(train_loss[sliding_window*i:(i+1)*sliding_window])/sliding_window)\n",
    "        cumulative_array_valid_loss.append(sum(valid_loss[sliding_window*i:(i+1)*sliding_window])/sliding_window)\n",
    "    # plt.plot(range(len(cumulative_array_train_loss)),cumulative_array_train_loss)\n",
    "    plt.plot(range(len(cumulative_array_valid_loss)),cumulative_array_valid_loss)\n",
    "    plt.show()\n",
    "    print(\"final train loss:\",cumulative_array_train_loss[-1],\"\\nminimum train loss:\",min(cumulative_array_train_loss))\n",
    "    print(\"final valid loss:\",cumulative_array_valid_loss[-1],\"\\nminimum valid loss:\",min(cumulative_array_valid_loss))\n",
    "    \n",
    "\n",
    "smoothed_convergence_curve(train_array_loss,val_loss_array,3000)\n",
    "\n",
    "# Will be used when validating after every weight update in training the network\n",
    "# def accuracy_curve(accuracy_array):\n",
    "#   plt.plot(range(len(accuracy_array)),accuracy_array)\n",
    "# accuracy_curve(predicted_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
